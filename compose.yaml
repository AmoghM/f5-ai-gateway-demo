services:

  open-webui-protected:
    container_name: open-webui-protected
    image: ghcr.io/open-webui/open-webui
    environment:
      WEBUI_AUTH: False
      WEBUI_NAME: Protected
      OPENAI_API_BASE_URL: http://aigw:4141/v1
      ENABLE_OLLAMA_API: false
      OPENAI_API_KEY: 42
    ports:
      - 9092:8080
    volumes:
      - ./service/open-webui/protected_webui.db:/app/backend/data/webui.db
    networks:
      - aigw

  open-webui-unprotected:
    container_name: open-webui-unprotected
    image: ghcr.io/open-webui/open-webui
    environment:
      WEBUI_AUTH: False
      WEBUI_NAME: Un-Protected
      OPENAI_API_BASE_URL: http://ollama:11434/v1
      ENABLE_OLLAMA_API: false
      OPENAI_API_KEY: 42
    ports:
      - 9093:8080
    volumes:
      - ./service/open-webui/unprotected_webui.db:/app/backend/data/webui.db
    networks:
      - aigw

  aigw:
    container_name: aigw
    image: private-registry.f5.com/aigw/aigw:v1.1.0
    env_file: "aigw-jwt.env"
    command:
      - start
      - /etc/aigw.yaml
      - --exporter-enable
      - --exporter-type=stdout
    ports:
      - 8081:4141
      - 8082:8080
    volumes:
      - ./inbound-config.yaml:/etc/aigw.yaml
      - ./aigw.openai.secret:/etc/openai
    networks:
      - aigw
    depends_on:
      aigw-processors-f5:
        condition: service_healthy

  aigw-processors-f5:
    container_name: aigw-processors-f5
    image: private-registry.f5.com/aigw/aigw-processors-f5:v1.1.0
    environment:
      TLS_ENABLED: false
      ENABLE_GPU: false
    restart: on-failure
    volumes:
      - ./service/aigw-processors-f5/healthcheck.py:/var/tmp/healthcheck.py
    ports:
      - 8002:8000
    networks:
      - aigw
    healthcheck:
      test: python /var/tmp/healthcheck.py
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 5s
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  aigw-processors-demo:
    container_name: aigw-processors-demo
    image: megamattzilla/ai-gateway-sdk-demo:user-prompt-v1.0
    restart: on-failure
    ports:
      - 8043:8000
    networks:
      - aigw
  
  orca-safety-processor:
    container_name: orca-safety-processor
    build: ./orca_processor
    env_file: ./orca_processor/.env
    ports:
      - 8001:8001
    networks:
      - aigw

## Uncomment the following lines to enable the F5 Labs processor
  # aigw-processors-f5-labs:
  #   container_name: aigw-processors-f5-labs
  #   image: private-registry.f5.com/aigw/processor-labs/prompt-guard:v0.0.1
  #   environment:
  #     TLS_ENABLED: false
  #     ENABLE_GPU: true
  #   restart: on-failure
  #   ports:
  #     - 8001:8001
  #   networks:
  #     - aigw
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: 1
  #             capabilities: [gpu]

  ollama:
    image: ollama/ollama:latest
    ports:
      - 11435:11434
    volumes:
      - ./service/ollama:/model_files  # Mount the directory with the ModelFile
    entrypoint: ["/bin/sh", "/model_files/run_ollama.sh"] # Loading the custom Modelfile
    container_name: ollama
#    pull_policy: always
    tty: true
    restart: always
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    networks:
      - aigw
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

networks:
  aigw:


